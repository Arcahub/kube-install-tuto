<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>How to deploy a Kubernetes cluster</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="A guide to deploy a Kubernetes cluster for a course at Ynov School">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="k8s-components.html"><strong aria-hidden="true">2.</strong> Understanding Kubernetes Components</a></li><li class="chapter-item expanded "><a href="infrastructure.html"><strong aria-hidden="true">3.</strong> Infrastructure Provisioning</a></li><li class="chapter-item expanded "><a href="installation/installation.html"><strong aria-hidden="true">4.</strong> Kubernetes installation with kubeadm</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="installation/controlplane-node.html"><strong aria-hidden="true">4.1.</strong> Setting up control plane node</a></li><li class="chapter-item expanded "><a href="installation/worker-node.html"><strong aria-hidden="true">4.2.</strong> Setting up worker node</a></li><li class="chapter-item expanded "><a href="installation/conclusion.html"><strong aria-hidden="true">4.3.</strong> Installation conclusion</a></li></ol></li><li class="chapter-item expanded "><a href="understanding.html"><strong aria-hidden="true">5.</strong> Understanding what we have done</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="understanding/static-pods.html"><strong aria-hidden="true">5.1.</strong> Static pods</a></li><li class="chapter-item expanded "><a href="understanding/certificates.html"><strong aria-hidden="true">5.2.</strong> Certificates</a></li></ol></li><li class="chapter-item expanded "><a href="first-deployment.html"><strong aria-hidden="true">6.</strong> Creating our first deployment (advanced)</a></li><li class="chapter-item expanded "><a href="scheduler.html"><strong aria-hidden="true">7.</strong> Playing with the scheduler</a></li><li class="chapter-item expanded "><a href="version-upgrade.html"><strong aria-hidden="true">8.</strong> Upgrading cluster version</a></li><li class="chapter-item expanded "><a href="create-config.html"><strong aria-hidden="true">9.</strong> Create a config for a developer (advanced)</a></li><li class="chapter-item expanded "><a href="bonus.html"><strong aria-hidden="true">10.</strong> Bonus</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">How to deploy a Kubernetes cluster</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to this tutorial about Kubernetes. This tutorial is intended for people who want to learn how to install and configure a Kubernetes cluster. It is also intended for people who want to understand how Kubernetes works.</p>
<p>This tutorial is based on the <a href="https://kubernetes.io/docs">official documentation of Kubernetes</a>. The documentation is very good, so I recommend you to read it if you want to learn more about Kubernetes or if you need help with a specific topic.</p>
<p>This course is divided into the following sections:</p>
<ul>
<li><a href="introduction.html">Introduction</a>: This page</li>
<li><a href="k8s-components.html">Kubernetes Components</a>: Kubernetes components</li>
<li><a href="infrastructure.html">Infrastructure Provisioning</a>: How to provision infrastructure for Kubernetes</li>
<li><a href="installation.html">Installation</a>: How to install Kubernetes</li>
<li><a href="understanding.html">Understanding what we have done</a>: Understanding what we have done</li>
<li><a href="version-upgrade.html">Upgrading cluster version</a>: How to upgrade Kubernetes cluster version</li>
<li><a href="create-user.html">Create a user for a developer</a>: How to create a user for a developer</li>
<li><a href="bonus.html">Bonus</a>: Bonus</li>
</ul>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>This course is written to be given in a school so it will be using Scaleway as a cloud provider. However, you can follow this tutorial without using Scaleway. You will just need to provision your own infrastructure.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="understanding-kubernetes-components"><a class="header" href="#understanding-kubernetes-components">Understanding Kubernetes Components</a></h1>
<p>Before starting any work, let's review how Kubernetes is designed and what are the components that compose it.</p>
<h2 id="kubernetes-architecture"><a class="header" href="#kubernetes-architecture">Kubernetes Architecture</a></h2>
<p><a href="https://kubernetes.io/docs/concepts/overview/components/">source</a></p>
<p>Kubernetes is a distributed system composed of multiple components. The following diagram shows the architecture of a Kubernetes cluster.</p>
<p><img src="./images/components-of-kubernetes.svg" alt="Kubernetes Architecture" /></p>
<p>The Kubernetes cluster consists of two types of resources:</p>
<ul>
<li>The <strong>control plane</strong> coordinates the cluster</li>
<li>The <strong>worker nodes</strong> are the machines that run applications</li>
</ul>
<p>A basic Kubernetes cluster has two worker nodes and one control plane node. The control plane manages the worker nodes and the pods running on the worker nodes through the Kubernetes API.</p>
<h2 id="kubernetes-control-plane"><a class="header" href="#kubernetes-control-plane">Kubernetes Control Plane</a></h2>
<p>As we said earlier, the control plane is responsible for managing the worker nodes and the pods running on the worker nodes. The control plane consists of the following components:</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a></li>
<li><a href="https://etcd.io/">etcd</a></li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a></li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a></li>
</ul>
<p>We will go through each of these components in the following sections.</p>
<h3 id="kube-apiserver"><a class="header" href="#kube-apiserver">kube-apiserver</a></h3>
<p>The kube-apiserver is the component that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. The API server is responsible for retrieving and updating data in the cluster. The API server is the only component that talks to the etcd cluster. The API server is responsible for:</p>
<ul>
<li>Exposing the Kubernetes API</li>
<li>Authenticating requests</li>
<li>Authorizing requests</li>
<li>Scheduling pods</li>
<li>Managing the cluster state</li>
</ul>
<h3 id="etcd"><a class="header" href="#etcd">etcd</a></h3>
<p>etcd is a distributed key-value store that provides a reliable way to store data across a cluster of machines. It's used by Kubernetes as backing store for all cluster data. The etcd cluster is used by the API server to store the state of the cluster.</p>
<h3 id="kube-scheduler"><a class="header" href="#kube-scheduler">kube-scheduler</a></h3>
<p>The kube-scheduler is responsible for scheduling pods on worker nodes. The scheduler watches for newly created pods that have no node assigned. For every pod that the scheduler discovers, the scheduler becomes responsible for finding the best node for that pod to run on. The scheduler reaches this decision taking into account the resources available on the node and any relevant constraints. The scheduler makes the decision based on a set of predefined policies. The default scheduler provided with Kubernetes is <code>default-scheduler</code>. You can create your own scheduler and use it instead of the default scheduler. The default scheduler looks at the following factors when scheduling a pod:</p>
<ul>
<li>Node affinity</li>
<li>Node anti-affinity</li>
<li>Pod affinity</li>
<li>Pod anti-affinity</li>
<li>Taints and tolerations</li>
<li>Node and pod resource requests and limits</li>
<li>Node conditions</li>
<li>Pod priority</li>
<li>Inter-pod affinity and anti-affinity</li>
<li>Node labels</li>
</ul>
<h3 id="kube-controller-manager"><a class="header" href="#kube-controller-manager">kube-controller-manager</a></h3>
<p>The kube-controller-manager is a daemon that embeds the core control loops shipped with Kubernetes. In Kubernetes, a control loop is a non-terminating loop that regulates the state of the cluster. The controller manager runs all the control loops in separate processes. The following controllers are included in the kube-controller-manager:</p>
<ul>
<li>Node Controller: Responsible for noticing and responding when nodes go down.</li>
<li>Replication Controller: Responsible for maintaining the correct number of pods for every replication controller object in the system.</li>
<li>Endpoints Controller: Populates the Endpoints object (that is, joins Services &amp; Pods).</li>
<li>Service Account &amp; Token Controllers: Create default accounts and API access tokens for new namespaces.</li>
</ul>
<h2 id="kubernetes-worker-nodes"><a class="header" href="#kubernetes-worker-nodes">Kubernetes Worker Nodes</a></h2>
<p>The worker nodes are the machines that run applications. The worker nodes consist of the following components:</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet</a></li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a></li>
</ul>
<h3 id="kubelet"><a class="header" href="#kubelet">kubelet</a></h3>
<p>The kubelet is the primary &quot;node agent&quot; that runs on each node. The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in the PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.</p>
<h3 id="kube-proxy"><a class="header" href="#kube-proxy">kube-proxy</a></h3>
<p>The kube-proxy is responsible for network proxying. The kube-proxy maintains network rules on the nodes. These network rules allow network communication to your Pods from network sessions inside or outside your cluster. The kube-proxy uses the operating system packet filtering layer if there is one available. Otherwise, kube-proxy forwards the traffic itself. The kube-proxy is responsible for:</p>
<ul>
<li>Load balancing connections across the different Pods</li>
<li>Maintaining network rules on the nodes</li>
<li>Forwarding traffic to the right Pod</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="infrastructure-provisionning"><a class="header" href="#infrastructure-provisionning">Infrastructure provisionning</a></h1>
<p>To deploy our Kubernetes cluster, we need to have some VMs available. To do this in this course, we will be using <a href="https://www.scaleway.com">Scaleway</a>.</p>
<p>At the root of this repository, you will find a <code>terraform</code> folder. It contains the source <a href="https://www.terraform.io/">Terraform</a> files that will provide our infrastructure.</p>
<h2 id="tldr"><a class="header" href="#tldr">TL;DR</a></h2>
<p>To install the infrastructure you will need to have a scaleway account and the <a href="https://github.com/scaleway/scaleway-cli">Scaleway CLI</a> installed and <a href="https://github.com/scaleway/scaleway-cli">setup</a></p>
<p>With the Scaleway CLI setup, you can then run the following command from the root of the repository:</p>
<pre><code class="language-bash">    cd terraform
    terraform init
    terraform plan
    terraform apply -var=&quot;project_name=&lt;your project name&gt;&quot; -var=&quot;project_id=&lt;your project id&gt;&quot;
</code></pre>
<p>You must provide the Scaleway project id and a custom project name.</p>
<p>Terraform will ask you to validate the creation of the infrastructure, press <code>yes</code> and wait for the infrastructure to be created.</p>
<p>To connect to the instances follow the instructions in the <a href="infrastructure.html#connection-to-the-instances">last section of this page</a></p>
<h2 id="step-by-step"><a class="header" href="#step-by-step">Step by step</a></h2>
<p>// TODO</p>
<h2 id="connection-to-the-instances"><a class="header" href="#connection-to-the-instances">Connection to the instances</a></h2>
<p>To connect to the instances, we will use the public gateway that is configured with a ssh bastion.</p>
<p>To get the ips of the instances and the public gateway you can run the following command to get outputs from terraform:</p>
<pre><code class="language-bash">    terraform output
</code></pre>
<p>To connect with ssh to any instance, you can use the following command:</p>
<pre><code class="language-bash">ssh -J bastion@&lt;public gateway_ip&gt;:61000 root@&lt;instance_ip&gt;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kubernetes-installation-with-kubeadm"><a class="header" href="#kubernetes-installation-with-kubeadm">Kubernetes installation with kubeadm</a></h1>
<p>In this section, we will install a Kubernetes cluster using <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/">kubeadm</a>.</p>
<p>The following steps are expected to be run on all nodes in the cluster. So first follow <em><strong>all</strong></em> the steps on the control plane node and then repeat the steps on all worker nodes.</p>
<p>So first ssh on the control plane node and then follow the steps.</p>
<h2 id="prepare-the-node"><a class="header" href="#prepare-the-node">Prepare the node</a></h2>
<p>Before installing Kubernetes components, we need to prepare the node by enabling the required kernel network modules.</p>
<pre><code class="language-bash">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
</code></pre>
<p><a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#install-and-configure-prerequisites">source</a></p>
<h2 id="cri-container-runtime-interface"><a class="header" href="#cri-container-runtime-interface">CRI (Container Runtime Interface)</a></h2>
<p>Kubernetes need a container runtime to run containers. Kubernetes defines an interface called the Container Runtime Interface (CRI). The CRI is an interface which allows Kubernetes to use a wide variety of container runtimes, without the need to recompile Kubernetes.</p>
<p>Kubernetes support multiple CRI implementations, in this course we will use <a href="https://containerd.io/">containerd</a> that is the default CRI for Kubernetes.</p>
<h3 id="installing-containerd"><a class="header" href="#installing-containerd">Installing containerd</a></h3>
<p>To install containerd we will need to add the docker repository to the package manager.</p>
<pre><code class="language-bash"># Install required packages for https repository
sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl
# Add Docker’s official GPG key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
# Add Docker repository
sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;
# Update package manager index
sudo apt-get update
</code></pre>
<p>Now we can install containerd.</p>
<pre><code class="language-bash">sudo apt-get install -y containerd.io
</code></pre>
<p>Now we need to configure containerd to use the systemd cgroup driver.</p>
<pre><code class="language-bash">sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml
</code></pre>
<p>Find the section <code>[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]</code> in the <code>/etc/containerd/config.toml</code> file and change the <code>SystemdCgroup</code> value to <code>true</code>.</p>
<p>Finally, you can restart the containerd service.</p>
<pre><code class="language-bash">sudo systemctl restart containerd
</code></pre>
<p><a href="https://docs.docker.com/engine/install/ubuntu/">source</a></p>
<h3 id="test-containerd-installation"><a class="header" href="#test-containerd-installation">Test containerd installation</a></h3>
<p>To test the installation of containerd we can run the following command.</p>
<pre><code class="language-bash">sudo ctr images pull docker.io/library/hello-world:latest
sudo ctr run --rm docker.io/library/hello-world:latest hello-world
sudo ctr images rm docker.io/library/hello-world:latest
</code></pre>
<p>If you see the following output, then the installation is successful.</p>
<pre><code class="language-bash">Hello from Docker!
This message shows that your installation appears to be working correctly.
</code></pre>
<h3 id="installing-kubeadm-kubelet-and-kubectl"><a class="header" href="#installing-kubeadm-kubelet-and-kubectl">Installing kubeadm, kubelet and kubectl</a></h3>
<p>To install kubeadm, kubelet and kubectl we will use ubuntu package manager (apt).</p>
<pre><code class="language-bash"># Install required packages for https repository
sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl gpg
# Add Kubernetes’s official GPG key
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
# Add Kubernetes repository
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
# Update package manager index
sudo apt-get update
# Install kubeadm, kubelet and kubectl with the exact same version or else components could be incompatible
sudo apt-get install -y kubelet=1.32.6-1.1 kubeadm=1.32.6-1.1 kubectl=1.32.6-1.1
# Hold the version of the packages
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre>
<p>The last line is very important because we don't want the Kubernetes components to be updated automatically by the package manager when running <code>apt-get upgrade</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-control-plane-node"><a class="header" href="#setting-up-control-plane-node">Setting up control plane node</a></h1>
<p>To init the control plane node with kubeadm simply ssh on the node and run the following command:</p>
<pre><code class="language-bash">sudo kubeadm init
</code></pre>
<p>Wait for it to finish.</p>
<h2 id="what-does-kubeadm-init-do"><a class="header" href="#what-does-kubeadm-init-do">What does kubeadm init do?</a></h2>
<p>The <code>kubeadm init</code> command does a lot of things:</p>
<ul>
<li>Install the necessary packages on the system</li>
<li>Create a Kubernetes configuration file</li>
<li>Create static pods for the control plane components</li>
<li>Generate certificates and keys for the cluster</li>
<li>Generate a kubeconfig for the control plane components</li>
<li>Generate a token to be used by the worker nodes to join the cluster</li>
<li>Generate a kubeconfig for the admin user</li>
</ul>
<h2 id="setup-kubeconfig"><a class="header" href="#setup-kubeconfig">Setup kubeconfig</a></h2>
<p>To be able to use kubectl we need to setup the kubeconfig file.
This configuration file is created by kubeadm and is located at <code>/etc/kubernetes/admin.conf</code>.
This enables us to interact with our Kubernetes cluster, currently with only one node as the admin user.</p>
<p>Run the following commands to copy the configuration file to the default location and change the ownership of the file to the current user:</p>
<pre><code class="language-bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<h2 id="check-that-control-plane-is-up"><a class="header" href="#check-that-control-plane-is-up">Check that control plane is up</a></h2>
<p>We now have a control plane node up and running. To check that everything is working, you can run the following command:</p>
<pre><code class="language-bash">kubectl get nodes
</code></pre>
<p>You should see something like this:</p>
<pre><code class="language-bash">NAME           STATUS      ROLES           AGE   VERSION
controlplane   NotReady    control-plane   10m   v1.25.0
</code></pre>
<p>The control plane node should be in the <code>NotReady</code> state yet. This is normal, we will fix this in the next section.</p>
<h2 id="install-cni-plugin"><a class="header" href="#install-cni-plugin">Install CNI plugin</a></h2>
<p>The control plane node is not ready because the pods are not able to communicate with each other.</p>
<p>If you run the following command:</p>
<pre><code class="language-bash">kubectl get pods --all-namespaces
</code></pre>
<p>You will see something like this:</p>
<pre><code class="language-bash">NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   coredns-74ff55c5b-2q2q2                    0/1     Pending   0          10m
kube-system   coredns-74ff55c5b-4q4q4                    0/1     Pending   0          10m
kube-system   etcd-controlplane                          1/1     Running   0          10m
kube-system   kube-apiserver-controlplane                1/1     Running   0          10m
kube-system   kube-controller-manager-controlplane       1/1     Running   0          10m
kube-system   kube-proxy-2q2q2                           1/1     Running   0          10m
kube-system   kube-scheduler-controlplane                1/1     Running   0          10m
</code></pre>
<p>You can see that the <code>coredns</code> pods are not ready and are in the <code>Pending</code> state. This is because the pods are waiting for a network to be available.</p>
<p>To fix this, we need to install a <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Container Network Interface (CNI) plugin</a>. A CNI plugin is a network plugin that will allow pods to communicate with each other.</p>
<p>We will use <a href="https://www.weave.works/oss/net/">Weave Net</a> in this course.</p>
<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.2/manifests/calico.yaml
</code></pre>
<p>Wait for the weave-net pods to be ready:</p>
<pre><code class="language-bash">kubectl -n kube-system wait pod -l k8s-app=calico-kube-controllers --for=condition=Ready --timeout=-1s
kubectl get pods -l k8s-app=calico-kube-controllers -n kube-system
</code></pre>
<h2 id="check-that-control-plane-node-is-ready"><a class="header" href="#check-that-control-plane-node-is-ready">Check that control plane node is ready</a></h2>
<p>To check that everything is working you can run the following command:</p>
<pre><code class="language-bash">kubectl get nodes
</code></pre>
<p>You should see something like this:</p>
<pre><code class="language-bash">NAME           STATUS   ROLES    AGE   VERSION
controlplane   Ready    master   10m   v1.25.0
</code></pre>
<p>The control plane node should be in the <code>Ready</code> state now.</p>
<p>We can also check that the <code>coredns</code> pod is now running:</p>
<pre><code class="language-bash">kubectl get pods --all-namespaces
</code></pre>
<p>You should see something like this:</p>
<pre><code class="language-bash">NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   coredns-74ff55c5b-2q2q2                    1/1     Running   0          10m
kube-system   etcd-controlplane                          1/1     Running   0          10m
kube-system   kube-apiserver-controlplane                1/1     Running   0          10m
kube-system   kube-controller-manager-controlplane       1/1     Running   0          10m
kube-system   kube-proxy-2q2q2                           1/1     Running   0          10m
kube-system   kube-scheduler-controlplane                1/1     Running   0          10m
</code></pre>
<h2 id="join-worker-nodes"><a class="header" href="#join-worker-nodes">Join worker nodes</a></h2>
<p>In the next section, we will make our worker nodes join the cluster.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-worker-node"><a class="header" href="#setting-up-worker-node">Setting up worker node</a></h1>
<p>On the control plane node create a token to join the worker node to the cluster.</p>
<p>Run this command on the control plane node.</p>
<pre><code class="language-bash">kubeadm token create --print-join-command
</code></pre>
<p>Copy the output of the command and run it on the worker node.</p>
<pre><code class="language-bash">kubeadm join ...
</code></pre>
<h2 id="check-that-worker-node-is-up"><a class="header" href="#check-that-worker-node-is-up">Check that worker node is up</a></h2>
<p>To check that the worker node is up and running you can run the following command on the control plane node:</p>
<pre><code class="language-bash">kubectl get nodes
</code></pre>
<p>You should see something like this:</p>
<pre><code class="language-bash">NAME           STATUS   ROLES                  AGE   VERSION
controlplane   Ready    control-plane,master   10m   v1.25.0
workernode     Ready    &lt;none&gt;                 10m   v1.25.0
</code></pre>
<p>We can add a label to add the worker role to the node.</p>
<pre><code class="language-bash">kubectl label node &lt;node-name&gt; node-role.kubernetes.io/worker=worker
</code></pre>
<h2 id="what-about-the-cni-"><a class="header" href="#what-about-the-cni-">What about the CNI ?</a></h2>
<p>For the installation of the control plane node we needed to install a CNI plugin for the node to be ready, but for the worker node we didn't. Can you explain why ?</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation-conclusion"><a class="header" href="#installation-conclusion">Installation conclusion</a></h1>
<p>You should now have a working Kubernetes cluster with a control plane node and two worker nodes. You can start playing with it and deploy your first application.</p>
<p>In the next section we will explain what we have done during the installation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="understanding-what-we-have-done"><a class="header" href="#understanding-what-we-have-done">Understanding what we have done</a></h1>
<p>So far, we have installed a Kubernetes cluster. We have also installed some tools to manage our cluster. In this section, we will try to understand what we have done.</p>
<p>You should have a Kubernetes cluster with 3 nodes. One node is the control plane node and the other two are worker nodes.</p>
<p>By running <code>kubectl get nodes</code> you should see something like this:</p>
<pre><code class="language-bash">NAME           STATUS   ROLES                  AGE   VERSION
controlplane   Ready    control-plane,master   10m   v1.25.0
workernode     Ready    worker                 10m   v1.25.0
workernode2    Ready    worker                 10m   v1.25.0
</code></pre>
<h2 id="what-we-have-done"><a class="header" href="#what-we-have-done">What we have done</a></h2>
<p>Let's review what we have done in the installation section.</p>
<h3 id="packages"><a class="header" href="#packages">Packages</a></h3>
<p>We installed a bunch of tools. We installed <code>kubeadm</code>, <code>kubectl</code>, <code>kubelet</code> and <code>containerd</code>.</p>
<p>You should already know <code>kubectl</code>, it is the command line tool to manage Kubernetes and <code>kubeadm</code> is a tool to manage a Kubernetes cluster.</p>
<p>Now let's talk about <code>kubelet</code> and <code>containerd</code>. We already mentioned <code>kubelet</code> in the previous section. It is a service that runs on each node of the cluster and is responsible for running containers. To run containers, it uses a container runtime. In our case, we use <code>containerd</code>.</p>
<p>We will explain more about <code>kubelet</code> and <code>containerd</code> in the <a href="create-a-deployment.html">create a deployment</a> section.</p>
<h3 id="kubeadm-init"><a class="header" href="#kubeadm-init">Kubeadm init</a></h3>
<p>After installing all the packages on the control plane node, we ran <code>kubeadm init</code> and we ended up with a Kubernetes cluster with only one node in not ready state. That was quite easy, right?</p>
<p>Let's explain a bit what kubeadm init does:</p>
<ul>
<li>It creates all needed certificates and keys that will be used to secure the cluster. (We will talk about that in the <a href="understanding/certificates.html">certificates</a> section)</li>
<li>It creates all needed configuration files for the control plane components. Just like the configuration file <code>/etc/kubernetes/admin.conf</code> we used to connect to the cluster with <code>kubectl</code> all components need their own configuration file to interact with the cluster.</li>
<li>It creates a static pod manifest for the control plane components. We will talk about static pods in the <a href="understanding/static-pods.html">static pods</a> section.</li>
</ul>
<p>That's the principals steps of <code>kubeadm init</code>. If you want more details, you can check the <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/">kubeadm init documentation</a>.</p>
<p>We already explained about the CNI plugin in the <a href="installation.html">installation</a> section, so we will not talk about it here. If you want more details, you can check the <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">CNI documentation</a>.</p>
<h3 id="kubeadm-join"><a class="header" href="#kubeadm-join">Kubeadm join</a></h3>
<p>After running <code>kubeadm init</code> on the control plane node, we ended up with a Kubernetes cluster with only one node. We need to add the other two nodes to the cluster.</p>
<p>To make a node join the cluster, we need to run <code>kubeadm join</code> with a token. The token is a secret that is used to authenticate the node to the cluster. The token is created when we run <code>kubeadm init</code> on the control plane node, but we can create tokens anytime we want like we did in the <a href="installation.html">installation</a> section.</p>
<p>In difference to <code>kubeadm init</code>, <code>kubeadm join</code> does less work since the control plane is already up and running. It will only create a kubelet configuration file and start the kubelet service with a secure identity for the node.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="static-pods"><a class="header" href="#static-pods">Static pods</a></h1>
<p>We just installed our Kubernetes cluster, and we can already see some pods running in the <code>kube-system</code> namespace. But where did these pods come from ? How did they get created ? How did they get scheduled on the nodes ?</p>
<p>These pods are called static pods. Static pods are pods that are managed directly by the kubelet daemon.</p>
<h2 id="what-are-static-pods-"><a class="header" href="#what-are-static-pods-">What are static pods ?</a></h2>
<p>As we said earlier, static pods are pods that are managed directly by the <code>kubelet</code> daemon. <code>kubelet</code> will watch a specific directory on the host file system every 20s (default value). If a file is created in this directory, <code>kubelet</code> will try to create a pod based on the file. If the file is deleted, <code>kubelet</code> will delete the pod.</p>
<p>The default static pod directory is <code>/etc/kubernetes/manifests</code>.</p>
<p>Run the following command to see the content of the static pod directory :</p>
<pre><code class="language-bash">sudo ls /etc/kubernetes/manifests
</code></pre>
<p>This command should return the following output :</p>
<pre><code class="language-bash">etcd.yaml
kube-apiserver.yaml
kube-controller-manager.yaml
kube-scheduler.yaml
</code></pre>
<p>You can check that these files match the pods that are running in the <code>kube-system</code> namespace :</p>
<pre><code class="language-bash">kubectl get pods --namespace=kube-system
</code></pre>
<p>You can also have a look at one of these files :</p>
<pre><code class="language-bash">sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml
</code></pre>
<h2 id="playing-with-static-pods"><a class="header" href="#playing-with-static-pods">Playing with static pods</a></h2>
<p>Let's play a bit with static pods to understand how they work.</p>
<p>Let's try to destroy the <code>kube-apiserver</code> pod only with <code>kubectl</code>. To do that you will need to identify the name of the pod:</p>
<pre><code class="language-bash">kubectl delete pod &lt;kube-apiserver-pod-name&gt; --namespace=kube-system
</code></pre>
<p>But if you check the pods again, you will see that the <code>kube-apiserver</code> pod is still running :</p>
<pre><code class="language-bash">kubectl get pods --namespace=kube-system
</code></pre>
<p>Why ? Because <code>kubelet</code> is still watching the static pod directory and will recreate the pod if it is deleted.</p>
<p>Let's try to delete the <code>kube-apiserver</code> pod file :</p>
<pre><code class="language-bash"># We only move the file to another location to be able to restore it later, what's important is that the file is deleted from the static pod directory
sudo mv /etc/kubernetes/manifests/kube-apiserver.yaml ~/kube-apiserver.yaml
</code></pre>
<p>Now the <code>kube-apiserver</code> is gone and how can we test that ? Try to run any kubectl command and you will endup with an error since kubectl can't contact the API server.</p>
<pre><code class="language-bash">kubectl get pods --namespace=kube-system
</code></pre>
<p>Let's restore the file :</p>
<pre><code class="language-bash">sudo mv ~/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml
</code></pre>
<p>And now, the <code>kube-apiserver</code> pod is back (it can take a few minutes to come back) :</p>
<pre><code class="language-bash">kubectl get pods --namespace=kube-system
</code></pre>
<p>We can also create a new pod file in the static pod directory :</p>
<pre><code class="language-bash">sudo tee /etc/kubernetes/manifests/nginx.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-test
  namespace: default
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
EOF
</code></pre>
<p>And now, the <code>nginx</code> pod is running :</p>
<pre><code class="language-bash">kubectl get pods --namespace=default
</code></pre>
<p>Let's clean up :</p>
<pre><code class="language-bash">sudo rm /etc/kubernetes/manifests/nginx.yaml
</code></pre>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>In this article, we saw how static pods work and where are located Kubernetes components manifests. Static pod are never used for anything else than managing these components, but you may have to modify their manifest.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="certificates"><a class="header" href="#certificates">Certificates</a></h1>
<p>As we said previously <code>kubeadm init</code> generates a set of certificates. All these certificates are stored in <code>/etc/kubernetes/pki</code>. This directory is used by Kubernetes to store certificates and keys. But what are these certificates for ?</p>
<h2 id="pki-public-key-infrastructure"><a class="header" href="#pki-public-key-infrastructure">PKI (Public Key Infrastructure)</a></h2>
<p>Kubernetes uses a PKI to secure the communication between components. PKI is a set of cryptographic tools that are used to generate, store and distribute certificates. The PKI used by Kubernetes is based on the PKI used by <code>etcd</code>. The PKI is composed of 3 main elements:</p>
<ul>
<li>Certificate Authority(CA) certificates</li>
<li>Components certificates</li>
<li>Users certificates</li>
</ul>
<h2 id="certificate-authorityca-certificates"><a class="header" href="#certificate-authorityca-certificates">Certificate Authority(CA) certificates</a></h2>
<p>The first certificate generated is the CA certificate. CA mean Certificate Authority. It is the root certificate that is used to sign all the other certificates. That means that we can validate a certificate by checking if it has been signed by the CA certificate.</p>
<p>This certificate is stored in <code>/etc/kubernetes/pki/ca.crt</code> and the private key in <code>/etc/kubernetes/pki/ca.key</code>. The private key is used to sign the other certificates.</p>
<h2 id="components-certificates"><a class="header" href="#components-certificates">Components certificates</a></h2>
<p>There is a pair of certificate and private key for each component of Kubernetes. The list of components is:</p>
<ul>
<li>kube-apiserver</li>
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
<li>kube-proxy</li>
<li>kubelet</li>
</ul>
<h2 id="etcd-certificates"><a class="header" href="#etcd-certificates">Etcd certificates</a></h2>
<p>Etcd has its own PKI. The list of certificates is:</p>
<ul>
<li>etcd-ca</li>
<li>etcd-server</li>
<li>etcd-peer</li>
<li>etcd-healthcheck-client</li>
</ul>
<h2 id="enable-authentication-for-kubelet-advanced"><a class="header" href="#enable-authentication-for-kubelet-advanced">Enable authentication for kubelet (advanced)</a></h2>
<p>The <code>kubelet</code> is the agent that runs on each node. It is responsible for starting and stopping containers. It also exposes an API but by default it is not secured. This means that anyone can access the API. Your task is to secure <code>kubelet</code> by enabling certificate authentication.</p>
<h2 id="certificate-management-with-kubeadm"><a class="header" href="#certificate-management-with-kubeadm">Certificate management with kubeadm</a></h2>
<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">source</a></p>
<p>Certificates expire after a given time. This time is called the certificate lifetime. The default lifetime for the component's certificates is 1 year and for the CA certificate is 10 years. The rotation of the certificates is done using <code>kubeadm</code>.</p>
<p>You can check the expiration date of the certificates using the following command:</p>
<pre><code class="language-bash">kubeadm certs check-expiration
</code></pre>
<p>You can renew all the certificates using the following command:</p>
<pre><code class="language-bash">kubeadm certs renew all
</code></pre>
<p>You can renew a specific certificate using the following command:</p>
<pre><code class="language-bash">kubeadm certs renew &lt;certificate-name&gt;
</code></pre>
<p>Example for the <code>kube-apiserver</code> certificate:</p>
<pre><code class="language-bash">kubeadm certs renew kube-apiserver
</code></pre>
<p>After renewing the certificates you need to restart the components that use them. To do so you can move the manifests of the static pods like we saw in the previous chapter.</p>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>Certificates are an important part of Kubernetes since they are the reasons we are able to have node to node communications over public networks. In the next chapter we will see in details what happens when we create a deployment.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="creating-our-first-deployment"><a class="header" href="#creating-our-first-deployment">Creating our first deployment</a></h1>
<p>Now that we have a working cluster, we can deploy our first application. But you should already have deployed something on Kubernetes, right? So to make it more interesting, I will explain you what happens behind the scenes when you run <code>kubectl create deployment nginx --image=nginx --replicas=3</code>.</p>
<p><a href="https://github.com/jamiehannaford/what-happens-when-k8s">source</a></p>
<h2 id="client-side"><a class="header" href="#client-side">Client side</a></h2>
<p>So let's get started. The firt things happening when you run <code>kubectl create deployment nginx --image=nginx --replicas=3</code> is that the client will read the file and perform client side validation. It will check if the file is a valid YAML file and if it contains the required fields. If it doesn't, it will return an error.</p>
<p>Then the client will create an HTTP request for the API server that will contains the YAML object. To do so, it will read the kubeconfig file to get the API server URL and the certificate to authenticate to the API server.</p>
<p>Finally, the client will send the request to the API server.</p>
<h2 id="api-server"><a class="header" href="#api-server">API server</a></h2>
<p>When the API server receive the request, it will first authenticate the client using the certificate. Then it will check if the request is authorized by checking the RBAC rules linked to the user. If the request is authorized, it will check if the object is valid. It will check if the object is a valid Kubernetes object and if it contains the required fields. If it doesn't, it will return an error.</p>
<p>As the last defense, the API server will check if the object is valid against the <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-are-they">admission controllers</a>. We won't go into details about the admission controllers but they are a set of plugins that can modify or reject objects. For example, the <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy</a> admission controller will check if the Pod is compliant with the PodSecurityPolicy.</p>
<p>At this point the request has been fully verified and the API server will store the object in the etcd database.</p>
<h2 id="controller-manager"><a class="header" href="#controller-manager">Controller manager</a></h2>
<p>Now that the object is stored in the etcd database, the next step is to create the corresponding Kubernetes object. A Deployment is a collection of ReplicaSets and a ReplicaSet is a collection of Pods. So the controller manager will create the ReplicaSet and the Pods and to do so, it will use Kubernetes' built-in controllers.</p>
<p>A controller is a loop that will check if the desired state stored in <code>etcd</code> is the same as the current state of the cluster. If it's not, it will try to reconcile the two states. For example, if you have a Deployment with 3 replicas and you delete one of the Pods, the controller will notice that the desired state is 3 Pods and the current state is 2 Pods and it will create a new Pod to reach the desired state.</p>
<p>So after a Deployment is stored in <code>etcd</code>, it is detected by the Deployment controller that will detect the <code>create</code> event on the Deployment. It will then that there are no ReplicaSets associated with the Deployment and it will create one. It will work similarly for the ReplicaSet and the Pods. If you want to know more about the controllers, you can read the <a href="https://kubernetes.io/docs/concepts/architecture/controller/">official documentation</a> and this article about <a href="http://borismattijssen.github.io/articles/kubernetes-informers-controllers-reflectors-stores">Kubernetes controllers</a>.</p>
<h2 id="scheduler"><a class="header" href="#scheduler">Scheduler</a></h2>
<p>After all the controllers have done their job, we have a Deployment, a ReplicaSet and a set of Pods stored in <code>etcd</code> but nothing is running on the nodes. To be more precise, the Pods are in the <code>Pending</code> state. The reason is that the Pods are not scheduled on any node.</p>
<p>The scheduler is responsible for scheduling the Pods on the nodes. It will check the Pod's requirements and try to find a node that can run the Pod. If it finds a node, it will update the Pod's <code>spec.nodeName</code> field and the Pod will be scheduled on the node.</p>
<p>For example, if you have a Pod that requires 1 CPU and 1GB of RAM, the scheduler will first filter the nodes with a series of predicates to assure they match the requirements of the Pod. Then it will rank the nodes with a series of priorities to find the best node. For example, it will try to find a node with the lowest number of Pods to reduce the risk of resource contention.</p>
<p>Again, if you want to know more about the scheduler, you can read the <a href="https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/">official documentation</a>.</p>
<h2 id="kubelet-1"><a class="header" href="#kubelet-1">Kubelet</a></h2>
<p>At this point, the Pods are scheduled on nodes with the <code>spec.nodeName</code> field set but they are still not running. And that's the job of <code>kubelet</code>.</p>
<p>As you know <code>kubelet</code> is the agent that runs on each node. It is responsible for translating the abstract Pod definition into a running container. To achieve that, it will query the API server to get the list of Pods by filtering on Pods that have the <code>spec.nodeName</code> field that correspond to the name of the node where the <code>kubelet</code> querying is running. Then it will detect change by comparing with its local cache. If there is a change, it will try to reconcile the two states.</p>
<p>For our example, the <code>kubelet</code> will see that the Pod is scheduled on the node and it will try to run it. To do so, it will use the container runtime to run the container. If the container is running, the Pod will be in the <code>Running</code> state.</p>
<h2 id="conclusion-2"><a class="header" href="#conclusion-2">Conclusion</a></h2>
<p>Well, that's it. This is the whole process that happens when you run <code>kubectl create deployment nginx --image=nginx --replicas=3</code>. Of course we can always go deeper and explain what happens in each step but if you have understood this article, you should have a good understanding of how Kubernetes works.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="playing-with-the-scheduler"><a class="header" href="#playing-with-the-scheduler">Playing with the scheduler</a></h1>
<p>In the last section we quickly saw that the scheduler assign the field <code>nodeName</code> in pod spec to make the pod run by <code>kubelet</code>. In this section we will play with the scheduler to see how it works.</p>
<h2 id="manual-scheduling"><a class="header" href="#manual-scheduling">Manual scheduling</a></h2>
<p>We can manually schedule a pod by setting the <code>nodeName</code> field in the pod spec. But first to be sure that the <code>scheduler</code> won't do anything for us, we will remove the <code>scheduler</code> from the cluster:</p>
<pre><code class="language-bash">sudo mv /etc/kubernetes/manifests/kube-scheduler.yaml /tmp
</code></pre>
<p>You can check that the scheduler is not running anymore:</p>
<pre><code class="language-bash">kubectl get pods -n kube-system
</code></pre>
<p>Now let's try to create a pod, we will first create the manifest to be able to modify it later:</p>
<pre><code class="language-bash">kubectl run nginx --image=nginx --dry-run=client -o yaml &gt; ~/nginx.yaml
</code></pre>
<p>And then create it:</p>
<pre><code class="language-bash">kubectl apply -f ~/nginx.yaml
</code></pre>
<p>The pod is created but is stuck in <code>Pending</code> state:</p>
<pre><code class="language-bash">kubectl get pods
</code></pre>
<p>We can see that the pod is waiting for a node to be assigned:</p>
<pre><code class="language-bash">NAME    READY   STATUS    RESTARTS   AGE
nginx   0/1     Pending   0          42s
</code></pre>
<p>Now let's deploy a second pod, first edit the manifest file, change the name of the pod, add a <code>nodeName</code> field with the name of a worker node as value and finally apply it:</p>
<pre><code class="language-bash">kubectl apply -f ~/nginx.yaml
</code></pre>
<p>The second pod will run without a problem while the other is still in pending state:</p>
<pre><code class="language-bash">kubectl get pods
</code></pre>
<pre><code class="language-bash">NAME     READY   STATUS    RESTARTS   AGE
nginx    0/1     Pending   0          14m
nginx2   1/1     Running   0          3m51s
</code></pre>
<p>Let's restore the scheduler:</p>
<pre><code class="language-bash">sudo mv /tmp/kube-scheduler.yaml /etc/kubernetes/manifests
</code></pre>
<p>And now the first pod while be scheduled:</p>
<pre><code class="language-bash">kubectl get pods
</code></pre>
<pre><code class="language-bash">NAME     READY   STATUS    RESTARTS   AGE
nginx    1/1     Running   0          15m
nginx2   1/1     Running   0          4m52s
</code></pre>
<p>We just proved that scheduling a pod is not magic, it only mean to set the <code>nodeName</code> field in the pod spec to assign the pod to a node. Of course the scheduler does not randomly choose a node, it has a logic to choose the best node for the pod and that's why it's an important component of the cluster.</p>
<p>Cleanup:</p>
<pre><code class="language-bash">kubectl delete pod nginx nginx2
</code></pre>
<h2 id="node-selector"><a class="header" href="#node-selector">Node selector</a></h2>
<p>Another way to schedule a pod on a specific node is to use a node selector. A node selector is a label that can be applied to a pod. If a node has a label that match the node selector, the scheduler will schedule the pod on that node.</p>
<p>Let's see how it works. First we will add a label to a node:</p>
<pre><code class="language-bash">kubectl label node k8s-node-1 node-type=worker
</code></pre>
<p>Now let's create a pod with a node selector:</p>
<pre><code class="language-bash">cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    node-type: worker
EOF
</code></pre>
<p>The pod is successfully created and running:</p>
<pre><code class="language-bash">kubectl get pods -o wide
</code></pre>
<pre><code class="language-bash">NAME      READY   STATUS    RESTARTS   AGE   IP           NODE
nginx     1/1     Running   0          2m    1.1.1.1      k8s-node-1
</code></pre>
<p>Cleanup:</p>
<pre><code class="language-bash">kubectl delete pod nginx
</code></pre>
<h2 id="affinity-and-anti-affinity"><a class="header" href="#affinity-and-anti-affinity">Affinity and anti-affinity</a></h2>
<p>Affinity and anti-affinity is a more advanced way to schedule a pod on a specific node. It allows to specify more complex rules to match a node. You can also specify if a rule is required or preferred that means that if the rule is not matched, the pod can still be scheduled on a node that doesn't match the rule.</p>
<p>You can also constrain scheduling based on labels on other pods running on the node rather than on labels on the node itself. This is called inter-pod affinity and anti-affinity.</p>
<h2 id="taints-and-tolerations"><a class="header" href="#taints-and-tolerations">Taints and tolerations</a></h2>
<p>One of the most important feature of the scheduler is the ability to schedule pods on specific nodes. This is done by using taints and tolerations. A taint is a label that can be applied to a node. A toleration is a label that can be applied to a pod. If a node has a taint, the scheduler will not schedule any pod on it unless the pod has a toleration for that taint.</p>
<p>Let's see how it works. By default we can't schedule a pod on the control plane node, that's because the control plane node has a taint:</p>
<pre><code class="language-bash">kubectl describe node &lt;control-plane-node-name&gt; | grep Taints
</code></pre>
<p>Let's try to create a pod that won't run on worker nodes but only on the control plane node:</p>
<pre><code class="language-bash">cat&lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
    containers:
    - name: nginx
      image: nginx
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Equal
      value: &quot;&quot;
    nodeSelector:
      node-role.kubernetes.io/control-plane: &quot;&quot;
EOF
</code></pre>
<p>The pod is successfully created and running:</p>
<pre><code class="language-bash">kubectl get pods
</code></pre>
<pre><code class="language-bash">NAME      READY   STATUS    RESTARTS   AGE
nginx     1/1     Running   0          2m
</code></pre>
<p>Cleanup:</p>
<pre><code class="language-bash">kubectl delete pod nginx
</code></pre>
<p>If we try to run the pod on the control plane node without the toleration, the pod will be stuck in <code>Pending</code> state:</p>
<pre><code class="language-bash">cat&lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
    containers:
    - name: nginx
      image: nginx
    nodeSelector:
      node-role.kubernetes.io/control-plane: &quot;&quot;
EOF
</code></pre>
<pre><code class="language-bash">kubectl get pods
</code></pre>
<pre><code class="language-bash">NAME      READY   STATUS    RESTARTS   AGE
nginx     0/1     Pending   0          2m
</code></pre>
<p>Cleanup:</p>
<pre><code class="language-bash">kubectl delete pod nginx
</code></pre>
<h2 id="pod-topology-spread-constraints"><a class="header" href="#pod-topology-spread-constraints">Pod topology spread constraints</a></h2>
<p>Pod topology spread constraints allow you to constrain the distribution of pods across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can be used to achieve high availability as well as efficient resource utilization.</p>
<p>To test this feature, create a deployment with 4 replicas that will act the same way as a daemonset. That means that there will one pod per node and since we have only 3 nodes, the last pod will be in pending state.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="upgrading-cluster-version"><a class="header" href="#upgrading-cluster-version">Upgrading cluster version</a></h1>
<p>I voluntarily made you install Kubernetes 1.32.0. This is because I wanted to show you how to upgrade a cluster. In this section, we will upgrade our cluster to Kubernetes 1.33.0.</p>
<h2 id="upgrade-control-plane"><a class="header" href="#upgrade-control-plane">Upgrade control plane</a></h2>
<p>To upgrade the control plane, we will use the <code>kubeadm upgrade</code> command. This command will upgrade the control plane components and the kubelet.</p>
<p>We will first upgrade <code>kubeadm</code> itself.</p>
<p>We need to switch the Kubernetes package repository to the next minor version. To do this, we will edit the Kubernetes APT repository file.</p>
<pre><code class="language-bash">sudo nano /etc/apt/sources.list.d/kubernetes.list
</code></pre>
<p>You should see a single line with the URL that contains your current Kubernetes minor version. For example, if you're using v1.32, you should see this:</p>
<pre><code class="language-bash">deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
</code></pre>
<p>Change the version in the URL to the next available minor release, for example:</p>
<pre><code class="language-bash">deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /
</code></pre>
<p>Save the file and exit your text editor.</p>
<pre><code class="language-bash">sudo apt-mark unhold kubeadm &amp;&amp; \
sudo apt update &amp;&amp; sudo apt remove -y kubeadm &amp;&amp; sudo apt install -y kubeadm=1.33.2-1.1 &amp;&amp; \
sudo apt-mark hold kubeadm
</code></pre>
<p>Now we can check if the upgrade is available.</p>
<pre><code class="language-bash">sudo kubeadm upgrade plan
</code></pre>
<p>If you see the following output then you can upgrade your cluster.</p>
<pre><code class="language-bash">[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/health] Checking API Server health: Healthy
[upgrade/health] Checking Node health: All Nodes are healthy
[upgrade/health] Checking Static Pod manifests exists on disk: All manifests exist on disk
[upgrade/health] Checking if control plane is Static Pod-hosted or Self-Hosted: Static Pod-hosted
[upgrade/health] Checking Static Pod manifests directory is empty: The directory is not empty
[upgrade/config] The configuration was checked to be correct:
[upgrade/config]      COMPONENT                 CURRENT        AVAILABLE
[upgrade/config]      API Server                v1.32.6        v1.33.2
[upgrade/config]      Controller Manager        v1.32.6        v1.33.2
[upgrade/config]      Scheduler                 v1.32.6        v1.33.2
[upgrade/config]      Kube Proxy                v1.32.6        v1.33.2
[upgrade/config]      CoreDNS                   1.8.0          1.8.0
[upgrade/config]      Etcd                      3.4.13-0       3.4.13-0
[upgrade/versions] Cluster version: v1.32.6
[upgrade/versions] kubeadm version: v1.33.2
[upgrade/versions] Latest stable version: v1.33.2
[upgrade/versions] Latest version in the v1.32 series: v1.32.6
[upgrade/versions] Latest experimental version: v1.34.0-alpha.0

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     1 x v1.32.6   v1.33.2
</code></pre>
<p>Now we can upgrade the cluster.</p>
<pre><code class="language-bash">kubeadm upgrade apply v1.33.2
</code></pre>
<p>If the upgrade is successful, you should see the following output.</p>
<pre><code class="language-bash">[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.33.x&quot;. Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
</code></pre>
<p>Before upgrading <code>kubelet</code>, we need to drain the control plane node.</p>
<pre><code class="language-bash">kubectl drain &lt;controlplane-node-name&gt; --ignore-daemonsets
</code></pre>
<p>You can now upgrade <code>kubelet</code></p>
<pre><code class="language-bash">sudo apt-mark unhold kubelet &amp;&amp; \
sudo apt update &amp;&amp; sudo apt remove -y kubelet &amp;&amp; sudo apt install -y kubelet=1.33.2-1.1 &amp;&amp; \
sudo apt-mark hold kubelet
</code></pre>
<p>Restart <code>kubelet</code></p>
<pre><code class="language-bash">sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre>
<p>Uncordon the control plane node</p>
<pre><code class="language-bash">kubectl uncordon controlplane
</code></pre>
<h2 id="upgrade-worker-nodes"><a class="header" href="#upgrade-worker-nodes">Upgrade worker nodes</a></h2>
<p>To upgrade the worker nodes the steps are similar to the control plane.</p>
<p>First upgrade <code>kubeadm</code></p>
<p>Again first, we need to switch the Kubernetes package repository to the next minor version. To do this, we will edit the Kubernetes APT repository file.</p>
<pre><code class="language-bash">sudo nano /etc/apt/sources.list.d/kubernetes.list
</code></pre>
<p>You should see a single line with the URL that contains your current Kubernetes minor version. For example, if you're using v1.32, you should see this:</p>
<pre><code class="language-bash">deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /
</code></pre>
<p>Change the version in the URL to the next available minor release, for example:</p>
<pre><code class="language-bash">deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /
</code></pre>
<p>Save the file and exit your text editor.</p>
<pre><code class="language-bash">sudo apt-mark unhold kubeadm &amp;&amp; \
sudo apt update &amp;&amp; sudo apt remove -y kubeadm &amp;&amp; sudo apt install -y kubeadm=1.33.2-1.1 &amp;&amp; \
sudo apt-mark hold kubeadm
</code></pre>
<p>Then upgrade the node</p>
<pre><code class="language-bash">sudo kubeadm upgrade node
</code></pre>
<p>If the upgrade is successful, you should see the following output.</p>
<pre><code class="language-bash">[upgrade/successful] SUCCESS! Your node was upgraded to &quot;v1.33.x&quot;. Enjoy!
</code></pre>
<p>Before upgrading <code>kubelet</code>, we need to drain the worker node, so go on the controlplane and run:</p>
<pre><code class="language-bash">kubectl drain &lt;worker-node-name&gt; --ignore-daemonsets
</code></pre>
<p>You can now upgrade <code>kubelet</code></p>
<pre><code class="language-bash">sudo apt-mark unhold kubelet &amp;&amp; \
sudo apt update &amp;&amp; sudo apt remove -y kubelet &amp;&amp; sudo apt install -y kubelet=1.33.2-1.1 &amp;&amp; \
sudo apt-mark hold kubelet
</code></pre>
<p>Restart <code>kubelet</code></p>
<pre><code class="language-bash">sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre>
<p>Uncordon the worker node</p>
<pre><code class="language-bash">kubectl uncordon &lt;worker-node-name&gt;
</code></pre>
<h2 id="check-that-cluster-is-up"><a class="header" href="#check-that-cluster-is-up">Check that cluster is up</a></h2>
<p>To check that the cluster is up and running you can run the following command on the control plane node:</p>
<pre><code class="language-bash">kubectl get nodes
</code></pre>
<p>You should see something like this:</p>
<pre><code class="language-bash">NAME            STATUS   ROLES                  AGE   VERSION
controlplane    Ready    control-plane,master   10m   v1.33.2
workernode      Ready    worker                 10m   v1.33.2
workernode2     Ready    worker                 10m   v1.33.2
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-a-config-for-a-developper"><a class="header" href="#create-a-config-for-a-developper">Create a config for a developper</a></h1>
<p>As a Kubernetes cluster administrator, you will need to create config files for the developpers of your company. This will allow them to access the cluster and deploy their applications.</p>
<p>The first step is to create a certificate for the developper. This certificate will be used to authenticate the developper to the cluster.</p>
<p>The certificate will be signed by the cluster CA. As we previously saw, the cluster CA is the certificate authority that is used to sign the certificates of the cluster components. The cluster CA is created when the cluster is created.</p>
<p>To create a certificate for a developper, you will need to create a certificate signing request (CSR). The CSR will be signed by the cluster CA. The CSR will contain the name of the developper. The name of the developper will be used to create a context for the developper.</p>
<p>Once you have successfully created the certificate, you will need to create a kubeconfig file for the developper. The kubeconfig file will contain the certificate of the developper and the address of the cluster. The kubeconfig file will be used by the developper to access the cluster.</p>
<p>Here is an example of a kubeconfig file for a developper named employee :</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: CA_LOCATION/ca.crt
    server: https://KUBERNETES_ADDRESS:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: employee
  name: employee
current-context: employee
users:
- name: employee
  user:
    client-certificate: DEVELOPER_CERTIFICATE_LOCATION/employee.crt
    client-key: DEVELOPER_CERTIFICATE_LOCATION/employee.key
</code></pre>
<p>In the <code>client-certificate</code> and <code>client-key</code> fields, you will need to replace <code>DEVELOPER_CERTIFICATE_LOCATION</code> with the location of the certificate of the developper. You can also directly copy the content of the <code>employee.crt</code> and <code>employee.key</code> files in the <code>client-certificate</code> and <code>client-key</code> fields.</p>
<p>Same things for the <code>certificate-authority</code> field. You will need to replace <code>CA_LOCATION</code> with the location of the cluster CA or you can also directly copy the content of the <code>ca.crt</code> file in the <code>certificate-authority</code> field.</p>
<p>In the <code>server</code> field, you will need to replace <code>KUBERNETES_ADDRESS</code> with the address of the cluster.</p>
<p>Once you have successfully created the kubeconfig file, you will need to give it to the developper. The developper will then be able to use the kubeconfig file to access the cluster.</p>
<h2 id="concrete-example"><a class="header" href="#concrete-example">Concrete example</a></h2>
<p>Let's say that you are the cluster administrator of a cluster named <code>kubernetes</code>. You will need to create a certificate for a developper named <code>employee</code>. You will then need to create a kubeconfig file for the developper. You will then need to give the kubeconfig file to the developper.</p>
<p>To create the certificate, you will need to ssh into the control plane node. You will then need to run the following command :</p>
<pre><code class="language-bash">openssl genrsa -out employee.key 2048
</code></pre>
<p>This command will create the private key of the developper. The private key will be used to sign the certificate signing request of the developper.</p>
<p>Then you will need to run the following command :</p>
<pre><code class="language-bash">openssl req -new -key employee.key -out employee.csr -subj &quot;/CN=employee&quot;
</code></pre>
<p>This command will create the certificate signing request of the developper. The certificate signing request will be signed by the cluster CA. The certificate signing request will contain the name of the developper. The name of the developper will be used to create a context for the developper.</p>
<p>Then you will need to run the following command :</p>
<pre><code class="language-bash">openssl x509 -req -in employee.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out employee.crt -days 500
</code></pre>
<p>This command will create the certificate of the developper. The certificate will be signed by the cluster CA. The certificate will contain the name of the developper. The name of the developper will be used to create a context for the developper.</p>
<p>Then you will need to run the following command :</p>
<pre><code class="language-bash">kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=https://KUBERNETES_ADDRESS:6443 --kubeconfig=/tmp/employee.kubeconfig
</code></pre>
<p>This command will create a cluster entry in the kubeconfig file. The cluster entry will contain the address of the cluster. The address of the cluster will be used to access the cluster.</p>
<p>Then you will need to run the following command :</p>
<pre><code class="language-bash">kubectl config set-credentials employee --client-certificate=employee.crt --client-key=employee.key --embed-certs=true --kubeconfig=/tmp/employee.kubeconfig
</code></pre>
<p>This command will create a user entry in the kubeconfig file. The user entry will contain the certificate of the developper. The certificate of the developper will be used to authenticate the developper to the cluster.</p>
<p>Then you will need to run the following command :</p>
<pre><code class="language-bash">kubectl config set-context employee --cluster=kubernetes --user=employee --kubeconfig=/tmp/employee.kubeconfig
</code></pre>
<p>This command will create a context entry in the kubeconfig file. The context entry will contain the name of the developper. The name of the developper will be used to create a context for the developper.</p>
<p>Then you will need to run the following command :</p>
<pre><code class="language-bash">kubectl config use-context employee --kubeconfig=/tmp/employee.kubeconfig
</code></pre>
<p>This command will set the current context of the kubeconfig file to the context of the developper.</p>
<p>Then you will need to run the following command :</p>
<pre><code class="language-bash">kubectl config view --flatten --minify --kubeconfig=/tmp/employee.kubeconfig &gt; ~/employee.kubeconfig
</code></pre>
<p>This command will flatten the kubeconfig file. The flattened kubeconfig file will be easier to read.</p>
<p>Then you will need to give the <code>employee.kubeconfig</code> file to the developper. The developper will then be able to use the kubeconfig file to access the cluster.</p>
<h2 id="test-the-kubeconfig-file"><a class="header" href="#test-the-kubeconfig-file">Test the kubeconfig file</a></h2>
<p>To test the kubeconfig file, you can use it with the <code>kubectl</code> command. You will need to run the following command :</p>
<pre><code class="language-bash">kubectl get pods --kubeconfig=employee.kubeconfig
</code></pre>
<p>This command should return the following error :</p>
<pre><code class="language-bash">Error from server (Forbidden): pods is forbidden: User &quot;employee&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>
<p>This error means that the developper is not authorized to list pods in the default namespace. This is normal because we have not given the developper any permissions. We will see how to give the developper some permissions in the next section.</p>
<p>But you can see that we have successfully authenticated against the API server. This means that the kubeconfig file is working.</p>
<h2 id="add-permissions-to-the-developper"><a class="header" href="#add-permissions-to-the-developper">Add permissions to the developper</a></h2>
<p>The developper will be able to access the cluster but he will not be able to do anything. The developper will not have any permissions. You will need to give the developper some permissions.</p>
<p>To give the developper some permissions, you will need to create a role. You will then need to create a role binding. You will then need to give the role binding to the developper.</p>
<p>To create the role, you will need to run the following command :</p>
<pre><code class="language-bash">kubectl create role developer --verb=get,list,watch --resource=pods --namespace=default
</code></pre>
<p>This command will create a role named <code>developer</code>. The role will allow the developper to get, list and watch pods in the default namespace.</p>
<p>To create the role binding, you will need to run the following command :</p>
<pre><code class="language-bash">kubectl create rolebinding developer --role=developer --user=employee --namespace=default
</code></pre>
<p>This command will create a role binding named <code>developer</code>. The role binding will bind the role <code>developer</code> to the developper <code>employee</code> in the default namespace.</p>
<h2 id="test-the-permissions"><a class="header" href="#test-the-permissions">Test the permissions</a></h2>
<p>To test the permissions, you can use the kubeconfig file with the <code>kubectl</code> command. You will need to run the following command :</p>
<pre><code class="language-bash">kubectl get pods --kubeconfig=employee.kubeconfig
</code></pre>
<p>This command should return the following output :</p>
<pre><code class="language-bash">No resources found in default namespace.
</code></pre>
<p>This output means that the developper is authorized to list pods in the default namespace.</p>
<h2 id="conclusion-3"><a class="header" href="#conclusion-3">Conclusion</a></h2>
<p>In this article, we have seen how to create a kubeconfig file for a developper. We have also seen how to give the developper some permissions.</p>
<p>That was the last article, in the next one I will give you some bonus exercises you can do if you want to go further.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bonus"><a class="header" href="#bonus">Bonus</a></h1>
<p>Well, this is the end of this tutorial. I hope you enjoyed it and learned something. If you want to learn more about Kubernetes, I recommend you to read the official documentation. It is very good and you will find everything you need to know about Kubernetes.</p>
<p>For people who want to go further, I have a few suggestions:</p>
<ul>
<li><a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the hard way</a></li>
<li>Use <a href="https://github.com/kubernetes-sigs/kubespray">kubespray</a> to install Kubernetes</li>
<li>Test to setup a LoadBalancer with another cloud provider than Scaleway</li>
<li>Deploy an HA cluster with 5 nodes instead of 3 (2 control plane and 3 workers)</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
